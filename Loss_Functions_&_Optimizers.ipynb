{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions and Optimizers\n",
    "\n",
    "### Loss Function\n",
    "Now we will define our learning objective, which a function that accepts two arguments: The network's output and the desired output (prediction, actual) \n",
    "\n",
    "This will return to us a single number: How close the network's prediction is from the desired result (actual)\n",
    "\n",
    "Using the loss value, we calculate gradients of network parameters and adjust them to decrease this loss value, which pushes our model to better results in the future. \n",
    "\n",
    "Some common loss functions:\n",
    "* ```nn.MSELoss```: The mean square error between arguments, which is the standard loss for regression problems\n",
    "* ```nn.BCELoss``` & ```nn.BCEWithLogits```: Binary cross-entropy loss. The first excepts a single probability value (the output of the sigmoid layer), while the second version assumes raw scores as input and applied ```sigmoid``` itself. \n",
    "* ```nn.CrossEntropyLoss``` & ```nn.LLLoss```: \"Maximumlikelihood criteria, which is used in multi-class classification problems. The first version expects raw scores for each class and applies ```LogSoftmax``` internally, while the second expects to have log probabilities as the input\n",
    "\n",
    "The result of the loss function shows the 'badness' of the network result relative to the target labels. It's result is a tensor of one single loss value. \n",
    "\n",
    "### Optimizers\n",
    "The basic responsibility is to take gradients of model parameters and change these parameters, in order to crease loss value. Decreasing the loss value will push our model towards desired outputs. \n",
    "\n",
    "```torch.optim``` package provides lots of popular optimizer implmentations some are:\n",
    "* ```SGD```: Vanilla stochastic gradient descent algorithm\n",
    "* ```RMSprop```: An optimizer, proposed by G. Hinton\n",
    "* ```Adagrad```: An adaptive gradient optimizer\n",
    "\n",
    "\n",
    "### Calculating Gradients\n",
    "Every tensor in this computational graph remembers its parent, so to calculate gradients for the whole network, all you need to do is to call ```backward()``` function on a loss function\n",
    "\n",
    "The result of ```backward()``` will be the unrolling of the graph of the performed computations and the calculations of gradients for every leaf tensor with ```require_grad=True```. These tensors are usually model's parameters: weights and bias of feed-forward networks, and convolutional filters. \n",
    "\n",
    "Every time a gradient is calculated, it is accumulated in the ```tensor.grad``` field, so one tensor can participate in a transformation multiple times and its gradient will properly summed up together. \n",
    "\n",
    "After ```loss.backward()``` call is finished, we have the gradients accumulated, and nwo its time for the optimizer to do its job: it takes all gradients from the parameters we've passed to it on construction and applies them. This is done in the ```step()``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
